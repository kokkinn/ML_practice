{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4008e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Adapted from lstm_text_generation.py in keras/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39e9a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 17:14:37.922439: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from urllib import request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbe36dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepairing data \n",
    "print('Prepairing data...\\n')\n",
    "url = 'https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt'\n",
    "text = request.urlopen(url).read().decode('utf8')\n",
    "\n",
    "# regex cleaning \n",
    "pattern1 = re.compile(r'[*\\[\\]â€™\\'\\\"`\\)\\(\\--]')\n",
    "pattern2 = re.compile(r'\\s{1,}')\n",
    "text = re.sub(pattern1, '', text)\n",
    "text = re.sub(pattern2, ' ', text)\n",
    "\n",
    "word_tokens = nltk.word_tokenize(text.lower())[0:5000] # is a list with all the words from a text \n",
    "# print(word_tokens)\n",
    "# print(sentense_tokens)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0602d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # frequency of words without removing stop words\n",
    "\n",
    "# freq = nltk.FreqDist(word_tokens)\n",
    "\n",
    "# plt.xticks(fontsize=6, rotation=90)\n",
    "# plt.grid()\n",
    "# plt.title('Frequency of some words', size=15)\n",
    "# freq.plot(80,cumulative=False, show=False)\n",
    "# plt.savefig('images/word_freq_with_stop_words.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70f523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# more filtering\n",
    "\n",
    "print('Filtering data...\\n')\n",
    "# removing stop words\n",
    "# stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "# stop_words.update([\",\", \".\", \":\", \";\", \"?\", \"!\", '*', '--', '[', ']', '``', '\\\"\\\"', \"\\'\\'\"])\n",
    "# word_tokens = [token for token in word_tokens if token not in stop_words] # words without stop words\n",
    "# print(word_tokens)\n",
    "\n",
    "\n",
    "# # lematization\n",
    "# lemmatizer = nltk.WordNetLemmatizer()\n",
    "# word_tokens = [lemmatizer.lemmatize(token.lower()) for token in word_tokens]\n",
    "# # print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365f0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # frequency of words with stop words removing\n",
    "\n",
    "# freq = nltk.FreqDist(word_tokens)\n",
    "# plt.xticks(fontsize=6, rotation=90)\n",
    "# plt.grid()\n",
    "# plt.title('Frequency of some words', size=15)\n",
    "# freq.plot(80,cumulative=False, show=False)\n",
    "# plt.savefig('images/word_freq_without_stop_words.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09ff248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags meaning parser\n",
    "\n",
    "dict_tags_meaning = dict()\n",
    "url = 'https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html'\n",
    "response = requests.get(url, 'html_parser')\n",
    "soup = BeautifulSoup(response.content)\n",
    "for child in list(soup.table.children)[2:]:\n",
    "    if isinstance(child, NavigableString):\n",
    "        continue\n",
    "    dict_tags_meaning[child.find_all('td')[1].text.strip()] = child.find_all('td')[2].text.strip()\n",
    "# print(dict_tags_meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0626cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags analyzis\n",
    "\n",
    "# df_tags = pd.DataFrame(nltk.pos_tag(word_tokens), columns=['Word', 'Tag'])\n",
    "\n",
    "# ser_tags_grouping = df_tags['Tag'].value_counts()\n",
    "# ser_tags_grouping.rename(index=dict_tags_meaning, inplace=True)\n",
    "# # print(ser_tags_grouping)\n",
    "# plt.xticks(fontsize=10, rotation=90)\n",
    "# plt.grid()\n",
    "# plt.title('Frequency of tags', size=15)\n",
    "# plt.plot(ser_tags_grouping)\n",
    "# plt.savefig('images/tags_freq.png', bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d50a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizing data\n",
    "\n",
    "print('Tokenizing data...\\n')\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(word_tokens)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "# tokenizer.word_index # is a dictionary with unique words and their numeric values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9da34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(word_tokens).replace(' ,', ',').replace(' .', '.').replace(' :', ':').replace(' ?', '?').replace(' ;', ';').replace('wa', 'was')\n",
    "# print(text)\n",
    "sentense_tokens = nltk.sent_tokenize(text) # is a list with all the sentenses from a text\n",
    "input_sequences = []\n",
    "for line in sentense_tokens:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "# pd.DataFrame(input_sequences).to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b5dd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating lookup tables\n",
    "\n",
    "# print('Creating lookup tables...\\n')\n",
    "# set_chars = set([c for c in text])\n",
    "# nb_chars = len(chars)\n",
    "# char2index = dict((c, i) for i, c in enumerate(set_chars))\n",
    "# index2char = dict((i, c) for i, c in enumerate(set_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecb45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Creating input and label text...\")\n",
    "# SEQLEN = 10\n",
    "# STEP = 1\n",
    "\n",
    "# input_chars = []\n",
    "# label_chars = []\n",
    "# for i in range(0, len(text) - SEQLEN, STEP):\n",
    "#     input_chars.append(text[i:i + SEQLEN])\n",
    "#     label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b52e8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vectorize the input and label chars\n",
    "# # Each row of the input is represented by seqlen characters, each\n",
    "# # represented as a 1-hot encoding of size len(char). There are\n",
    "# # len(input_chars) such rows, so shape(X) is (len(input_chars),\n",
    "# # seqlen, nb_chars).\n",
    "# # Each row of output is a single character, also represented as a\n",
    "# # dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n",
    "# # nb_chars).\n",
    "\n",
    "# print(\"Vectorizing input and label text...\")\n",
    "# X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "# y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "# for i, input_char in enumerate(input_chars):\n",
    "#     for j, ch in enumerate(input_char):\n",
    "#         X[i, j, char2index[ch]] = 1\n",
    "#     y[i, char2index[label_chars[i]]] = 1\n",
    "# np.zeros(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c70dc460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Build the model. We use a single RNN with a fully connected layer\n",
    "# # # to compute the most likely predicted output char\n",
    "# HIDDEN_SIZE = 128\n",
    "# BATCH_SIZE = 128\n",
    "# NUM_ITERATIONS = 25\n",
    "# NUM_EPOCHS_PER_ITERATION = 1\n",
    "# NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(SimpleRNN(500, return_sequences=False, input_shape=(max_sequence_len-1, 1), unroll=True))\n",
    "# model.add(Dense(total_words))\n",
    "# model.add(Activation(\"softmax\"))\n",
    "\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "# history = model.fit(xs, ys, epochs=30, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaaf6565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # We train the model in batches and test output generated at each step\n",
    "# for iteration in range(NUM_ITERATIONS):\n",
    "#     print(\"=\" * 50)\n",
    "#     print(\"Iteration #: %d\" % (iteration))\n",
    "#     model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "#     # testing model\n",
    "#     # randoml       y choose a row from input_chars, then use it to\n",
    "#     # generate text from model for next 100 chars\n",
    "#     test_idx = np.random.randint(len(input_chars))\n",
    "#     test_chars = input_chars[test_idx]\n",
    "#     print(\"Generating from seed: %s\" % (test_chars))\n",
    "#     print(test_chars, end=\"\")\n",
    "#     for i in range(NUM_PREDS_PER_EPOCH):\n",
    "#         Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "#         for i, ch in enumerate(test_chars):\n",
    "#             Xtest[0, i, char2index[ch]] = 1\n",
    "#         pred = model.predict(Xtest, verbose=0)[0]\n",
    "#         ypred = index2char[np.argmax(pred)]\n",
    "#         print(ypred, end=\"\")\n",
    "#         # move forward with test_chars + ypred\n",
    "#         test_chars = test_chars[1:] + ypred\n",
    "#     print()\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cfcd9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 17:14:51.610499: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3/3 [==============================] - 4s 161ms/step - loss: 4.1937 - accuracy: 0.0225\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 4.1241 - accuracy: 0.0449\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 4.0240 - accuracy: 0.0562\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - 0s 156ms/step - loss: 3.7260 - accuracy: 0.1011\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 3.3617 - accuracy: 0.1236\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 2.8868 - accuracy: 0.1236\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 2.4521 - accuracy: 0.3034\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 2.2870 - accuracy: 0.2584\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 1.9796 - accuracy: 0.3258\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 0s 161ms/step - loss: 1.7846 - accuracy: 0.3708\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 1.5620 - accuracy: 0.4382\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - 1s 173ms/step - loss: 1.3603 - accuracy: 0.5730\n",
      "Epoch 13/15\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 1.1260 - accuracy: 0.6854\n",
      "Epoch 14/15\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.9502 - accuracy: 0.8315\n",
      "Epoch 15/15\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 0.7554 - accuracy: 0.8876\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, total_words/2, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = False)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=15, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b2c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = history.history['accuracy']\n",
    "# loss = history.history['loss']\n",
    "# epochs = range(len(acc))\n",
    "# plt.plot(epochs, acc)\n",
    "# plt.title('Training accuracy')\n",
    "# plt.savefig('images/model_acc.png', bbox_inches='tight')\n",
    "# plt.plot(epochs, loss, 'b')\n",
    "# plt.title('Training loss')\n",
    "\n",
    "# plt.savefig('images/model_loss.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9eba403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down the wonderland adventures in wonderland lewis the millennium fulcrum edition 3 0 chapter down the rabbithole alice beginning to get tired\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"down the\"\n",
    "next_words = 20\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
    "\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break \n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e518a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
